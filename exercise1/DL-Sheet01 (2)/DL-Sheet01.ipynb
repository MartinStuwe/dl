{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7a20e8f-0cd6-40c1-ad2e-d1a37d538783",
   "metadata": {},
   "source": [
    "# Deep Learning - Winter Term 2023/2024\n",
    "\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "### Exercise Sheet 01\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05317f1b-462b-43e3-abda-1ea2d30d39e3",
   "metadata": {},
   "source": [
    "## 3.1) Autograd Implementation\n",
    "\n",
    "In the first exercise sheet, we will puzzle out the backpropagation in autograd. We believe, it will be fruitful for you, if you solve the first exercises and obtain an intution about how a computational graph works before you start the programming exercise. Here, you basically need to implement derivatives of certain function. Be careful with reoccurence of the same variable though!\n",
    "\n",
    "- We didn't write any tests for you because you can do it yourself! For instance, you could write down any equation you want and compute the gradients per hand and test your code.\n",
    "- ***Note:*** Try to think about the edge cases in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35edbd42-67c5-4d8a-b063-e102dba28908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will only use NumPy for this exercise\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888dfefc-01f5-4005-a1c7-dbd13251d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "As you probably know, Python is an object-oriented programming language.\n",
    "Therefore we will treat each node in the computational graph as an object.\n",
    "Here is the Node class, that creates object with certain information (i.e. scalar, name etc.)\n",
    "Each Node is an object and represents a node in the computational graph,\n",
    "whereas the comp. graph itself is a graph structure, composed of multiple nodes arranged in a systematic fashion.\n",
    "'''\n",
    "\n",
    "class Node:\n",
    "    \n",
    "    ### Initialization of the class ###\n",
    "    '''\n",
    "    scalar: given scalar -- only scalars will be used in this exercise\n",
    "    _children: child node in the graph ##_ in the beginning means it is an internal variable for the graph construction##\n",
    "    _op: applied operation in order to reach the scalar\n",
    "    name: string representation of the scalar\n",
    "    '''\n",
    "    def __init__(self, scalar, _children=(), _op='', name=''):\n",
    "        self.scalar = scalar\n",
    "        # Before computing the gradients, you always need to make sure that\n",
    "        # they are set to zero. To do that, you can basically initialize the nodes again.\n",
    "        self.grad = 0.0 # By default we assume the variable does not effect the function\n",
    "        self.name = name\n",
    "        self._backward = lambda: None # Meaning the function doesn't do anything by default\n",
    "        self._prev = set(_children) # Save the child node in a set.\n",
    "        self._op = _op\n",
    "    \n",
    "    # This function just makes the printing of the Node human-readable\n",
    "    def __repr__(self):\n",
    "        return f\"Node(name={self.name}, scalar={self.scalar}, grad={self.grad})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Node) else Node(other)\n",
    "        out = Node(self.scalar + other.scalar, (self, other), '+') # forward pass\n",
    "        \n",
    "        def _backward():\n",
    "            # TODO: implement the backward pass of this function\n",
    "            raise NotImplementedError()\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Node) else Node(other)\n",
    "        out = Node(self.scalar * other.scalar, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            # TODO: implement the backward pass of this function\n",
    "            raise NotImplementedError()\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def __pow__(self, other):\n",
    "        other = other if isinstance(other, Node) else Node(other)\n",
    "        out = Node(self.scalar**other.scalar, (self, other), f'**{other.scalar}')\n",
    "\n",
    "        def _backward():\n",
    "            # TODO: implement the backward pass of this function (be careful with the edge cases!)\n",
    "            raise NotImplementedError()\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.scalar\n",
    "        out = Node(np.exp(x), (self, ), 'exp')\n",
    "    \n",
    "        def _backward():\n",
    "            # TODO: implement the backward pass of this function\n",
    "            raise NotImplementedError()\n",
    "        out._backward = _backward\n",
    "    \n",
    "        return out\n",
    "\n",
    "    def sigmoid(self):\n",
    "        # TODO: implement the forward pass of this function\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "        def _backward():\n",
    "            # TODO: implement the backward pass of this function\n",
    "            raise NotImplementedError()\n",
    "        out._backward = _backward\n",
    "    \n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        # TODO: implement the forward pass of this function\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "        def _backward():\n",
    "            # TODO: implement the backward pass of this function\n",
    "            raise NotImplementedError()\n",
    "        out._backward = _backward\n",
    "    \n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        # TODO: implement the forward pass of this function\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        def _backward():\n",
    "            # TODO: implement the backward pass of this function\n",
    "            raise NotImplementedError()\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def log(self):\n",
    "        x = self.scalar\n",
    "        out = Node(np.nan if x < 0 else np.log(x), (self, ), 'log')\n",
    "        \n",
    "        def _backward():\n",
    "            # TODO: implement the backward pass of this function\n",
    "            raise NotImplementedError()\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def sqrt(self):\n",
    "        assert self.scalar > 0, \"No complex spaces here\"\n",
    "        x = self.scalar\n",
    "        out = Node(np.sqrt(x), (self, ), 'sqrt')\n",
    "        \n",
    "        def _backward():\n",
    "            # TODO: implement the backward pass of this function\n",
    "            raise NotImplementedError()\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def cos(self):\n",
    "        x = self.scalar\n",
    "        out = Node(np.cos(x), (self, ), 'cos')\n",
    "        \n",
    "        def _backward():\n",
    "            # TODO: implement the backward pass of this function\n",
    "            raise NotImplementedError()\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def sin(self):\n",
    "        x = self.scalar\n",
    "        out = Node(np.sin(x), (self, ), 'sin')\n",
    "        \n",
    "        def _backward():\n",
    "            # TODO: implement the backward pass of this function\n",
    "            raise NotImplementedError()\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # This function performs backpropagation for each node\n",
    "    # Nothing to implement here but you can spend some time to understand how\n",
    "    # the algorithm sorts the nodes.\n",
    "    def backward(self):\n",
    "        # topological order of the visited children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        # The gradient of the last operation wrt. itself is always 1.\n",
    "        # We set this first gradient here.\n",
    "        self.grad = 1.0\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "    \n",
    "    # Below are same necessary trivial definitions.\n",
    "    # Feel free to use these functions. No gradient computation\n",
    "    # is needed as they are implicitely implemented in prev. functions.\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**-1\n",
    "    \n",
    "# ----------------------------------------------------------------\n",
    "# Helper functions\n",
    "# ----------------------------------------------------------------\n",
    "    \n",
    "def trace_nodes(node):\n",
    "    '''\n",
    "    Builds a list of all nodes of a graph. The method is \n",
    "    only for inspection/debugging purposes. Feel free to use is if you need it.\n",
    "    '''\n",
    "    nodes = list()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.insert(0, v)\n",
    "            for child in (v._prev):\n",
    "                build(child)\n",
    "    build(node)\n",
    "    return nodes\n",
    "\n",
    "def print_graph(root):\n",
    "    '''\n",
    "    Produces a print out of the computational graph.\n",
    "    '''\n",
    "    nodes = trace_nodes(root)\n",
    "    for n in nodes:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357b0e1d-88dd-4bdb-bf18-ea36a6ecf75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we give a graph visualization algorithm for your computational graph.\n",
    "# You need to install the graphviz package: https://graphviz.org/\n",
    "# After the installation, you are good to go. Feel free to play around\n",
    "# with the visualization! For example, you can compare your own computational graph drawing.\n",
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_graph(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any scalar in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{ %s | scalar %s | grad %s }\" % (n.name, n.scalar, n.grad), shape='record')\n",
    "    if n._op:\n",
    "      # if this scalar is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b80d66-8754-45d4-b7ad-e96df239bcf8",
   "metadata": {},
   "source": [
    "### 3.2) Test your implementation\n",
    "\n",
    "Here, you need to implement the equations from the second exercise. You basically need to write down the intermediate values from ***2.a)***. The important part is that you need to make a gradient computation for these equations. As input scalars, you can choose whatever you want. However, first you need to make the computation per hand such that you can debug your code in case your implementation gives a different result. You can look at the gradient by printing either the node itself or __node.grad__\n",
    "\n",
    "Let's make a simple example. Here is a tiny equation:\n",
    "$$\n",
    "\\mathcal{Z} = a*b + c\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90794f7f-7f50-44d4-8b7a-edf2ee758197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the equation Z and compute the gradients here.\n",
    "a = Node(2.0,  name='a')\n",
    "b = Node(-3.0, name='b')\n",
    "c = Node(10.0, name='c')\n",
    "\n",
    "# Due to overloading the operators we can use the arithmetic\n",
    "# operators like +, * directly in expressions. Note that the\n",
    "# expression \n",
    "#\n",
    "#     a * b + c\n",
    "# \n",
    "# is equivalent to \n",
    "#\n",
    "#     a * b + c = a.__mul__(b).__add__(c)\n",
    "#\n",
    "# Moreover, it automatically creates intermediate nodes for\n",
    "# a * b and _ + c.\n",
    "#\n",
    "# In order to inspect every single piece of the graph, we \n",
    "# manually create intermediate nodes:\n",
    "\n",
    "Z_t1 = a * b; Z_t1.name='Z_t1'\n",
    "Z_t2 = Z_t1 + c; Z_t2.name='Z_t2'\n",
    "\n",
    "# We run the backward function to compute all the gradients starting from the end node.\n",
    "# The gradient of each node n is stored in n.grad\n",
    "Z_t2.backward()\n",
    "\n",
    "# Print the nodes to see their name, scalar and *most importantly* their gradients.\n",
    "print(c)\n",
    "print(a)\n",
    "print(b)\n",
    "print(Z_t1)\n",
    "print(Z_t2)\n",
    "print()\n",
    "\n",
    "# We could have written this more compact (as explained above):\n",
    "a.grad=0; b.grad=0; c.grad=0;\n",
    "Z = a * b + c; Z.name='Z'\n",
    "Z.backward()\n",
    "print_graph(Z)\n",
    "#\n",
    "# Notes:\n",
    "# - Due to the use of the data structure set, the nodes stored in\n",
    "#   every _prev variable are not arranged in the order they have been\n",
    "#   inserted -> this is why the printing order is c, a, b (don't worry\n",
    "#   about it).\n",
    "# - A repeated call of backward() will cause an accumulation of gradients\n",
    "#   within the respective nodes. \n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f3ac63-f393-4c47-bb1a-667f1d66cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the following one-liner, you can visualize the comp. graph\n",
    "draw_graph(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca05277f-7f9d-4b45-ad64-80c975122583",
   "metadata": {},
   "source": [
    "After understanding the previous example, you can start the exercise! We write down the equations here to make your life easier.\n",
    "$$\n",
    "\\mathcal{A} = \\sqrt{a + b + c^2} + \\log(a + b + c^2) + \\frac{a + b + c^2}{bc^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0689035c-73e6-45a5-a761-a825f9ea61be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the equation A and compute the gradients here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e3376-f18c-498f-8bf0-bdfa55110032",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{B} = \\sum_{i=1}^{3}(w_0 + w_1x_i - y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6f1f16-ab7c-4eff-a4ab-737dce3ea355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the equation B and compute the gradients here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81541cd-c318-498b-b9dc-4170fa2d8451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
